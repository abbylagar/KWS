{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio, torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import librosa\n",
    "from torchvision.transforms import ToTensor\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random wav file\n",
    "wav_file = \".\\\\checkpoints\\\\bed.wav\"\n",
    "waveform, sample_rate = torchaudio.load(wav_file)\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate,\n",
    "                                            n_fft=512,\n",
    "                                            win_length=None,\n",
    "                                            hop_length=256,\n",
    "                                            n_mels=40,\n",
    "                                            power=2.0)\n",
    "if waveform.shape[-1] < sample_rate:\n",
    "            waveform = torch.cat([waveform, torch.zeros((1, sample_rate - waveform.shape[-1]))], dim=-1)\n",
    "elif waveform.shape[-1] > sample_rate:\n",
    "    waveform = waveform[:,:sample_rate]\n",
    "\n",
    "\n",
    "data = ToTensor()(librosa.power_to_db(transform(waveform).squeeze().numpy(), ref=np.max))\n",
    "mel = torch.cat([data, torch.zeros(1,40,1)],dim=-1)\n",
    "mel = mel.unsqueeze(0)\n",
    "mels = rearrange(mel, 'b c (p1 h) (p2 w) ->b (p1 p2) (c h w)', p1=1,p2=32)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_transformer import LitTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitTransformer(num_classes=37, lr=args.lr, epochs=args.max_epochs, \n",
    "                        depth=args.depth, embed_dim=args.embed_dim, head=args.num_heads,\n",
    "                        patch_dim=4, seqlen=16,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = model.load_from_checkpoint(os.path.join('.\\\\checkpoints', \"kws_best_acc-v1.ckpt\"))\n",
    "model.eval()\n",
    "script = model.to_torchscript()\n",
    "\n",
    "# save for use in production environment\n",
    "model_path = os.path.join('.\\\\checkpoints', \"kws_best_acc-v1.pt\")\n",
    "torch.jit.save(script, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = ArgumentParser(description='PyTorch Transformer')\n",
    "    parser.add_argument('--depth', type=int, default=12, help='depth')\n",
    "    parser.add_argument('--embed_dim', type=int, default=80, help='embedding dimension')\n",
    "    parser.add_argument('--num_heads', type=int, default=4, help='num_heads')\n",
    "\n",
    "    parser.add_argument('--patch_num', type=int, default=32, help='patch_num')\n",
    "    parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, metavar='N',\n",
    "                        help='input batch size for training (default: )')\n",
    "    parser.add_argument('--max-epochs', type=int, default=35, metavar='N',\n",
    "                        help='number of epochs to train (default: 0)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate (default: 0.0)')\n",
    "\n",
    "    parser.add_argument('--accelerator', default='gpu', type=str, metavar='N')\n",
    "    parser.add_argument('--devices', default=1, type=int, metavar='N')\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, metavar='N')\n",
    "    parser.add_argument('--num_workers', default=2, type=int, metavar='N')\n",
    "\n",
    "    parser.add_argument(\"--no-wandb\", default=False, action='store_true')\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "            'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "            'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "            'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "idx_to_class = {v: k for k, v in CLASS_TO_IDX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_module = torch.jit.load(model_path)\n",
    "pred = torch.argmax(scripted_module(mels), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: , Prediction: off\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ground Truth: , Prediction: {idx_to_class[pred.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: visual\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: visual\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: two\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: nine\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: yes\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: on\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: yes\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: go\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: happy\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: on\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: marvin\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: no\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: yes\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: marvin\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: right\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: follow\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: cat\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: seven\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: nine\n",
      "int32\n",
      "float32\n",
      "torch.Size([1, 1, 40, 64])\n",
      "Ground Truth: , Prediction: unknown\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 16000  # Record in chunks of 1024 samples\n",
    "#sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "sample_format =pyaudio.paInt32\n",
    "channels = 1\n",
    "fs = 16000  # Record at 44100 samples per second\n",
    "seconds = 30\n",
    "filename = \"output3.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "g=[]\n",
    "#print(stream)\n",
    "# Store data in chunks for 3 seconds\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "\n",
    "    data = stream.read(chunk)\n",
    "#    print(\"data\")\n",
    "#    print(np.array(data))\n",
    "        # Save the recorded data as a WAV file\n",
    "   # samps = torch.frombuffer(data,dtype=torch.int32)\n",
    "    samps = np.frombuffer(data,dtype=np.int32)\n",
    "    print(samps.dtype)\n",
    "    maxi=max(abs(samps))\n",
    "    samps=samps/maxi\n",
    "    samps =samps.astype(np.float32)\n",
    "    print(samps.dtype)\n",
    "    samps=torch.from_numpy(samps)\n",
    "    \n",
    "    \n",
    "    mel = ToTensor()(librosa.power_to_db(transform(samps).squeeze().numpy(), ref=np.max))\n",
    "    #print(mel.size())\n",
    "    mel = torch.cat([mel, torch.zeros(1,40,1)],dim=-1)\n",
    "    mel = mel.unsqueeze(0)\n",
    "    print(mel.size())\n",
    "    mel = rearrange(mel, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=1, p2=32)\n",
    "    scripted_module = torch.jit.load(model_path)\n",
    "    pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "\n",
    "    print(f\"Ground Truth: , Prediction: {idx_to_class[pred.item()]}\")\n",
    "        \n",
    "    #print(torch.FloatTensor(data))\n",
    "    #rd_data = af.readframes(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "# Stop and close the stream \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "# Terminate the PortAudio interface\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "\n",
    "# Save the recorded data as a WAV file\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules.\n",
    "import tkinter\n",
    "import tkinter as tk\n",
    "import tkinter.messagebox\n",
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class InferAudio:\n",
    "\n",
    "    def __init__(self, chunk=16000, frmat=pyaudio.paInt16, channels=1, rate=16000, py=pyaudio.PyAudio()):\n",
    "\n",
    "        # Start Tkinter and set Title\n",
    "        self.main = tkinter.Tk()\n",
    "        self.collections = []\n",
    "        self.main.geometry('1000x300')\n",
    "        self.main.title('Record')\n",
    "        self.CHUNK = chunk\n",
    "        self.FORMAT = frmat\n",
    "        self.CHANNELS = channels\n",
    "        self.RATE = rate\n",
    "        self.p = py\n",
    "        self.frames = []\n",
    "        self.st = 1\n",
    "        self.stream = self.p.open(format=self.FORMAT, channels=self.CHANNELS, rate=self.RATE, input=True, frames_per_buffer=self.CHUNK)\n",
    "\n",
    "        # Set Frames\n",
    "        self.buttons = tkinter.Frame(self.main, padx=150, pady=50)\n",
    "        self.l1 = tkinter.Label(self.main, text=\"Click Start\",font=(\"Helvetica\", 50),fg='red')\n",
    "      \n",
    "\n",
    "        # Pack Frame\n",
    "        self.buttons.pack(fill=tk.BOTH)\n",
    "\n",
    "        self.l1.pack()   \n",
    "\n",
    "\n",
    "        # Start and Stop buttons\n",
    "        self.strt_rec = tkinter.Button(self.buttons, width=50, padx=10, pady=5, text='Start', command=lambda: self.start_record())\n",
    "        self.strt_rec.grid(row=0, column=0, padx=150, pady=5)\n",
    "        self.stop_rec = tkinter.Button(self.buttons, width=50, padx=10, pady=5, text='Stop', command=lambda: self.stop())\n",
    "        self.stop_rec.grid(row=1, column=0, columnspan=1, padx=150, pady=5)\n",
    "        \n",
    "        #self.l1 = Label(self.main, text=\"hi\")\n",
    "\n",
    "        \n",
    "        tkinter.mainloop()\n",
    "\n",
    "    def start_record(self):\n",
    "        self.st = 1\n",
    "        self.frames = []\n",
    "        #datacollect = []\n",
    "        stream = self.p.open(format=self.FORMAT, channels=self.CHANNELS, rate=self.RATE, input=True, frames_per_buffer=self.CHUNK)\n",
    "        maxi = 0\n",
    "        while self.st == 1:\n",
    "            data = stream.read(self.CHUNK)\n",
    "            self.frames.append(data)\n",
    "            #print(\"* recording\")\n",
    "            #inference   \n",
    "            samps = np.frombuffer(data,dtype=np.int16)\n",
    "            samps =samps.astype(np.float32)\n",
    "\n",
    "            #print(samps.dtype)\n",
    "            maxi_new=max(abs(samps))\n",
    "            \n",
    "            #update maximum\n",
    "            if maxi_new > maxi:\n",
    "                maxi = maxi_new\n",
    "            else: \n",
    "                maxi = maxi\n",
    "\n",
    "\n",
    "            samps=samps/maxi\n",
    "            #samps =samps.astype(np.float32)\n",
    "            #print(samps)\n",
    "            samps=torch.from_numpy(samps)\n",
    "            #print(samps.shape)\n",
    "            \"\"\"        \n",
    "            if samps.shape[-1] < self.RATE:\n",
    "                samps= torch.cat([samps, torch.zeros((1, self.RATE - samps.shape[-1]))], dim=-1)\n",
    "            elif samps.shape[-1] > self.RATE:\n",
    "                samps = samps[:,:self.RATE]\n",
    "\n",
    "            \"\"\"\n",
    "                    \n",
    "            #print(samps.size)\n",
    "            mel = ToTensor()(librosa.power_to_db(transform(samps).squeeze().numpy(), ref=np.max))\n",
    "           \n",
    "            #print(mel.size())\n",
    "            mel = torch.cat([mel, torch.zeros(1,40,1)],dim=-1)\n",
    "            mel = mel.unsqueeze(0)\n",
    "            #print(mel.size())\n",
    "            mel = rearrange(mel, 'b c (p1 h) (p2 w) -> b (p1 p2) (c h w)', p1=1, p2=32)\n",
    "            scripted_module = torch.jit.load(model_path)\n",
    "            pred = torch.argmax(scripted_module(mel), dim=1)\n",
    "\n",
    "            #print(f\"Ground Truth: , Prediction: {idx_to_class[pred.item()]}\")\n",
    "   \n",
    "            prediction = idx_to_class[pred.item()]\n",
    "\n",
    "            self.l1.config(text=str(prediction))\n",
    "            #time.sleep(0)\n",
    "            self.main.update()\n",
    "\n",
    "        stream.close()\n",
    "\n",
    "        wf = wave.open('./outputs/voice_record.wav', 'wb')\n",
    "        wf.setnchannels(self.CHANNELS)\n",
    "        wf.setsampwidth(self.p.get_sample_size(self.FORMAT))\n",
    "        wf.setframerate(self.RATE)\n",
    "        wf.writeframes(b''.join(self.frames))\n",
    "        wf.close()\n",
    " \n",
    "    def stop(self):\n",
    "       # print('stop recording')\n",
    "        self.st = 0\n",
    "        \n",
    "\n",
    "\n",
    "# Create an object of the ProgramGUI class to begin the program.\n",
    "guiAUD = InferAudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccef6d892591bb3ab92a365675b31270e1f831a19b434e3b6026351d8d2d41e5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('deeplearning2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
